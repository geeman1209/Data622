---
title: "DATA 622 FINAL PROJECT | Coutries diet & COVID-19 Mortality Rate"
author: "Abdellah Ait Elmouden | Gabriel Abreu |  Jered Ataky"
date: "12/10/2021"
output:
  html_document:
    code_folding: hide
    theme: cerulean
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  pdf_document:
    toc: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=F, warning=F)
```


```{r}
library(GGally)
library(ggplot2)
library(caret)
library(mice)
library(tidyverse)
library(DataExplorer)
library(MASS)
library(naniar)
library(kableExtra)
library(skimr)
library(randomForest)
library(fitdistrplus)
library(corrplot)
library(recipes)
```


## Introduction

Nutritional factors play a key role in both innate and adaptive immunity. Further, we have learned that individuals with comorbidities are disproportionately affected with severe COVID-19 disease and mortality. Obesity, type 2 diabetes, atherosclerotic cardiovascular disease, and hypertension are risk factors for severe COVID-19.The aetiology of these conditions is largely driven by poor nutrition and unfavorable lifestyle choices (eg, physical inactivity or sedentary behaviour) 

The goal of this analysis is to find out how a country’s diet correlates with its COVID-19 mortality rate. With different food cultures across the world, it would be interesting to see what are the food categories that can best predict a country’s rate of deaths.

## The Dataset

Data for this project was taken from this very interesting [kaggle](https://www.kaggle.com/mariaren/covid19-healthy-diet-dataset) dataset. the dataset combined data of different types of food, world population obesity and undernourished rate, and global COVID-19 cases count from around the world in order to learn more about how a healthy eating style could help combat the Corona Virus.

There are 5 files in the dataset:

- Fat_Supply_Quantity_Data.csv: percentage of fat intake from different food groups for 170 different countries.
- Food_Supply_Quantity_kg_Data.csv: percentage of food intake( in  kg  ) from different food groups for 170 different countries.
- Food_Supply_kcal_Data.csv: percentage of energy intake (in  kcal ) from different food groups for 170 different countries.
- Protein_Supply_Quantity_Data.csv: percentage of protein intake from different food groups for 170 different countries.
All of these files have, also, columns including obesity, undernourishment and COVID-19 cases as percentages of total population.
- Supply_Food_Data_Descriptions.csv: This dataset is obtained from FAO.org, and is used to show the specific types of food that belongs to each category for the above datasets.

In each of the 4 datasets above, they have calculated fat quantity, energy intake (kcal), food supply quantity (kg), and protein for different categories of food (all calculated as percentage of total intake amount). they've also added on the obesity and undernourished rate (also in percentage) for comparison. The end of the datasets also included the most up to date confirmed/deaths/recovered/active cases (also in percentage of current population for each country).

**Data Collection**

- Data for different food group supply quantities, nutrition values, obesity, and undernourished percentages were obtained from Food and Agriculture Organization of the United Nations [FAO website](http://www.fao.org/faostat/en/#home) To see the specific types of food included in each category from the FAO data.
- Data for population count for each country comes from Population Reference Bureau [PRB website](https://www.prb.org/).
- Data for COVID-19 confirmed, deaths, recovered and active cases are obtained from Johns Hopkins Center for Systems Science and Engineering [CSSE website](https://coronavirus.jhu.edu/map.html).
- The USDA Center for Nutrition Policy and Promotion diet intake guideline information can be found in [ChooseMyPlate.gov](https://www.choosemyplate.gov/).


## Methodology

Before applying machine learning models, We started first with the process to understand the data which is called Exploratory Data Anaysis(EDA). It refers to the process of initial investigation and analysis of the dataset in order to understand the distribution, anomality, correlation and other data characteristics .

In this project we especially focus on using tree-based algorithms such as random forest, gradient boosting and Cubist. Decision tree analysis it is a general, predictive modelling tool, and It is one of the most widely used and practical methods for supervised learning. our goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. After that we'll proceed with a Random forests analysis, it is commonly reported as the most accurate learning algorithm. and also because it reduces the variance seen in decision trees. Also to check for more comprehensive predictive accuracy we used a cubist algorithm and compare all three models results to demonstrate which one has the highest prediction
performance

## Data Exploration and Processing

### Data overview

```{r}
fat_data <- read.csv("./data/Fat_Supply_Quantity_Data.csv")
supply_kcal_data <- read.csv("./data/Food_Supply_kcal_Data.csv")
supply_kg_data <- read.csv("./data/Food_Supply_Quantity_kg_Data.csv")
protein_data <- read.csv("./data/Protein_Supply_Quantity_Data.csv")
```

After importing the files we have noticed that the datasets are similar, and we choose to focus on the easiest to understand: food_kcal. Caloric intake from specific food groups makes it easy to understand which food groups contribute the most to a country's diet and overall calorie count. Diets can be high caloric but traditional thinking dictates that diets rich in vegetables and fruits will be healthier than those that eat a large amount of meat and sugars. Also the Unit column only contains the % sign, indicating that all the column except the Population one are in percentages. It is important to know the unit used.

The data are organized by countries. There are 170 countries in these datasets. 32 variables as following:

```{r}
names(supply_kcal_data)
```

### Missing Data

Missing data in substantive research is common and can be problematic for structural equation modelling if not handled correctly. A well recognised statistical benchmark suggests datasets with missingness less than 5% is not a problem. Within this dataset we checked if our data variable had less than 5% missing data and therefore not be considered a problem to conduct further statistical tests. quick screening the data we can see that certain columns have some null values. The plot below generated using the R plot_missing function helps us understand that almost 4% of data are missing in the "Recovered", "Deaths", "Confirmed", ""Undernourished" and "Active" Variables. and almost 2% in the "Obesity", So still can imput the missing values using predictive mean matching method, that is a widely used statistical imputation method for missing values, so can get a better predictive accuracy.

```{r}
plot_missing(supply_kcal_data,  missing_only = TRUE)
```

#### Imputation**

```{r, echo=FALSE}
#Giving the countries with an undernourished percentage of <2.5, a flat 2. This eliminates the less than character and doesn't rely on 
#imputation to make up a new number. The imputed data would give a country a like the United States a percentage of 47.5, which can greatly
#affect how it is clustered
raw_data <- supply_kcal_data
raw_data$Undernourished[raw_data$Undernourished == "<2.5"] <- "2"

#Make the Undernourished column numeric
raw_data$Undernourished <- as.numeric(raw_data$Undernourished)
```

```{r, echo=FALSE}
#Use mice package to impute the missing data
imputed_data <- mice(raw_data, m=2, maxit = 2, method='cart', seed = 500)
complete_data <- complete(imputed_data, 2)
```

```{r, echo=FALSE}
complete_data2 <- complete_data %>% dplyr::select(c(-"Country",-"Unit..all.except.Population.", -"Confirmed",-"Active", -"Recovered"))
```


### Statistical Analyses

#### Numerical Variables Distribution

The statistical analysis included a prior investigation on the distribution of the independent and dependent variable (recovery from COVID-19), testing different types of probability distribution. 

First we will quickly display a broad overview of a data frame, using the skim function

```{r}
skim(complete_data2)
```

from the histogram plot below we can see that the majority of the predictors are skewed and all of the values for `Aquatic.Products..Other` equals 0, making it a prime candidate to eliminate from further analysis. This data set will benefit from a Box Cox transformation. 


```{r}
plot_histogram(complete_data2, ggtheme = theme_linedraw())
```

Some other graphical methods, maybe more helpful than the simple histogram. we used the fitdisrplus package in R to visualize the recovery variable data together with some possible theoretical distributions in a skewness-kurtosis space:

```{r}
plotdist(complete_data2$Re, histo = TRUE, demp = TRUE)
```
From the empirical density above, our distribution is right skewed and appears to be an exponential type of distribution. The Cullen and Frey Graph below is a good way to exempt some distributions by the parameters of skewness and kurtosis using the descdist function; The orange values around the blue (data) point are based on bootstrapping. From this Cullen and Frey Graph and the empirical graphs above, our choices for good fits would seem to be limited to the available distributions in the fitdistrplus package:

- Weibull
- gamma
- exponential


```{r}
library(fitdistrplus)
descdist(complete_data2$Recovered, boot=1000) 
```
```{r}

# I added the > 0 because the gamma dist doesn't allow zero values 
fw <- fitdist(complete_data2$Recovered[complete_data2$Recovered > 0], distr = "weibull", lower = 0.0)
fg <- fitdist(complete_data2$Recovered[complete_data2$Recovered > 0], distr = "gamma", lower = 0.0)
fe <- fitdist(complete_data2$Recovered[complete_data2$Recovered > 0], distr = "exp", lower = 0.0)
par(mfrow = c(2, 2))
plot.legend <- c("Weibull", "gamma", "expo")
denscomp(list(fw, fg, fe), legendtext = plot.legend)
qqcomp(list(fw, fg, fe), legendtext = plot.legend)
cdfcomp(list(fw, fg, fe), legendtext = plot.legend)
ppcomp(list(fw, fg, fe), legendtext = plot.legend)
```

From the plotted fitting metrics above, it appears that Weibull and gamma are the best contenders, but it seems a little unclear who is the clear winner form the density plot. Let’s take a closer look with a larger density plot for these two distributions.

```{r}
denscomp(list(fw, fg), legendtext = c("Weibull", "gamma"))
```
It seems that still both distribution fits this data the best. Let us confirm this against the Akaline’s and Bayesian Information Criterion (AIC and BIC), which will give a sort of rank to the goodness of fit models passed to it using the gofstat function as well as the Goodness-of-fit statistics, which give distances between the fits and the empirical data.

```{r}
gofstat(list(fw, fg))
```
Since the gamma distribution has the min AIC, BIC, and minimum goodness-of-fit statistics, we will choose the gamma distribution.




```{r, echo=FALSE}
#Select top 20 countries of different predictors (those usually associated with healthy or unhealthy diets) 
top_obese_countries <- complete_data %>% 
                       dplyr::select(Country, Obesity) %>%
                       arrange(desc(Obesity)) %>%
                       head(20)


top_deaths_countries <- complete_data %>% 
                        dplyr::select(Country, Deaths) %>%
                        arrange(desc(Deaths)) %>%
                        head(20)

top_veggies_countries <- complete_data %>% 
                        dplyr::select(Country, Vegetables) %>%
                        arrange(desc(Vegetables)) %>%
                        head(20)

top_meat_countries <- complete_data %>% 
                        dplyr::select(Country, Meat) %>%
                        arrange(desc(Meat)) %>%
                        head(20)

top_aniproducts_countries <- complete_data %>% 
                        dplyr::select(Country, Animal.Products) %>%
                        arrange(desc(Animal.Products)) %>%
                        head(20)

top_sugar_countries <- complete_data %>% 
                        dplyr::select(Country, Sugar...Sweeteners) %>%
                        arrange(desc(Sugar...Sweeteners)) %>%
                        head(20)
```


```{r}
par(mfrow=c(2,2))
ggplot(top_obese_countries, aes(x=Obesity, y=Country)) + geom_histogram(stat = "identity")
ggplot(top_deaths_countries, aes(x=Deaths, y=Country)) + geom_histogram(stat = "identity")
ggplot(top_veggies_countries, aes(x=Vegetables, y=Country)) + geom_histogram(stat = "identity")

```

```{r}
par(mfrow=c(2,2))
ggplot(top_aniproducts_countries, aes(x=Animal.Products, y=Country)) + geom_histogram(stat = "identity")
ggplot(top_meat_countries, aes(x=Meat, y=Country)) + geom_histogram(stat = "identity")
ggplot(top_sugar_countries, aes(x=Sugar...Sweeteners, y=Country)) + geom_histogram(stat = "identity")
```



## Data Manipulation

```{r, echo=FALSE}
#make a copy of the original data set 
raw_data <- supply_kcal_data
```


### Split the data

```{r}
#Creating a non-normalized and scaled data set (only BoxCox transformation, eliminate near zero variance and highly correlated variables)
Processed <- preProcess(complete_data2, method = c("BoxCox", "nzv","corr"))
Processed
```

```{r}
treeData <- predict(Processed, complete_data2)
```

```{r}
set.seed(150)
predictors <- subset(treeData, select = -Deaths)
Deaths <- subset(treeData, select="Deaths")
initsplit <- createDataPartition(Deaths$Deaths, p=0.8, list=FALSE)

#Create Training Data to tune the model
X.train <- predictors[initsplit,]
Y.train <- Deaths[initsplit,]

#Create testing data to evaluate the model
X.test <- predictors[-initsplit,]
Y.test <- Deaths[-initsplit,]
```

```{r}
#Creating a normalized, scaled data set along with the other transformations
Processed2 <- preProcess(complete_data2, method = c("center", "scale","BoxCox","nzv","corr"))
Processed2
```

```{r}
nrmData <- predict(Processed2, complete_data2)
```

```{r}
set.seed(150)

predictors2 <- subset(nrmData, select = -Deaths)
Deaths2 <- subset(nrmData, select="Deaths")
initsplit2 <- createDataPartition(Deaths2$Deaths, p=0.8, list=FALSE)

#Create Training Data to tune the model
X.train.cs <- predictors2[initsplit2,]
Y.train.cs <- Deaths2[initsplit2,]

#Create testing data to evaluate the model
X.test.cs <- predictors2[-initsplit2,]
Y.test.cs <- Deaths2[-initsplit2,]
```

## Modeling


### PCA 

#### Variable correlation

Before we start our PCA analysis we selected the variables that we are interested on and checked if a significant correlation among some of these variables. We want to create a list of the strongest correlations with diets for 2 covid states: deaths, and recovered. We will not use the active, and confirmed covid state.



```{r}
M<- cor(complete_data2[, c('Alcoholic.Beverages','Animal.fats', 'Eggs', 'Fish..Seafood', 'Fish..Seafood', 'Fruits...Excluding.Wine','Meat', 'Milk...Excluding.Butter', 'Starchy.Roots','Sugar...Sweeteners','Vegetal.Products','Vegetable.Oils','Vegetables','Recovered', 'Deaths')])

#M <- cor.test(eng_imputed)

head(round(M,2))
```
We also added significance test to the correlogram we shall compute the p-value. In the correlogram blow If the p-value is greater than 0.01 then it is an insignificant value for which the cells are either blank or crossed. 


```{r}
library(RColorBrewer)
cor.mtest <- function(mat, ...) 
{
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) 
  {
    for (j in (i + 1):n)
    {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}

p.mat <- cor.mtest(complete_data2[, c('Alcoholic.Beverages','Animal.fats', 'Eggs', 'Fish..Seafood', 'Fish..Seafood', 'Fruits...Excluding.Wine','Meat', 'Milk...Excluding.Butter', 'Starchy.Roots','Sugar...Sweeteners','Vegetal.Products','Vegetable.Oils','Vegetables','Recovered', 'Deaths')])

# Correlation 
M <- cor(complete_data2[, c('Alcoholic.Beverages','Animal.fats', 'Eggs', 'Fish..Seafood', 'Fish..Seafood', 'Fruits...Excluding.Wine','Meat', 'Milk...Excluding.Butter', 'Starchy.Roots','Sugar...Sweeteners','Vegetal.Products','Vegetable.Oils','Vegetables','Recovered', 'Deaths')])


# Specialized the insignificant value
# according to the significant level
corrplot(M, method="number", type = "upper", order = "hclust", 
         p.mat = p.mat, sig.level = 0.01, tl.col = "black")

```
From the plot we can see that when we compare death and recovered cases, the animal fat, animal products significantly higher in terms of correlation. Recovered cases diets have a lesser correlation to meat diet.

####  PCA Summary

```{r}
Data.pca1 <- prcomp(complete_data2, center=TRUE, scale.=TRUE)
summary(Data.pca1)
```
**Z=XV both give the same output**

```{r}
Data.pca1$x [,1:10] %>% head(1) 
```

**V matrix - Eigenvectors**

```{r}
head(Data.pca1$rotation)
```
**Principal components**

Since the principal components are orthogonal, there is no correlation whatsoever. The correlation plot is perfectly white, apart from autocorrelation.

```{r}
res1 <- cor(Data.pca1$x, method="pearson")
corrplot::corrplot(res1, method= "color", order = "hclust", tl.pos = 'n')
```

#### PCA exploration

The plot below shows what percent of variance has been explained for each number of principal components (aggregate variance explained).

```{r}
plot(summary(Data.pca1)$importance[3,])
```

#### Plots

**PC1-PC2**

```{r}
library(factoextra)
fviz_pca_var(Data.pca1,axes = c(1, 2))
```

**PC3-PC4**

```{r}
fviz_pca_var(Data.pca1,axes = c(3, 4))
```

**PC5-PC6**

```{r}
fviz_pca_var(Data.pca1,axes = c(5, 6))
```


**PC7-PC8**

```{r}
fviz_pca_var(Data.pca1,axes = c(7, 8))
```

##### Linear regression with principal components

Below are scatterplots showing the relation between PCs and y. Some of the principal components have a high correlation and others a lower correlation with the observed dependent variable.

**Scatterplots**

```{r}
par(mfrow=c(2,2))
pcs <- as.data.frame(Data.pca1$x)
plot(complete_data2$Deaths, pcs$PC1)
plot(complete_data2$Deaths, pcs$PC2)
plot(complete_data2$Deaths, pcs$PC3)
plot(complete_data2$Deaths, pcs$PC4)
```

#### PCR

In order to perform the regression, we are combining both the explanatory variables - PCs, and explained variable - y. In this moment the dimensionality reduction should take place. Here however we won’t be getting rid of any principal components since we want to compare the results with prc from pls package.

```{r}
ols.data <- cbind(complete_data2$Deaths, pcs)
```

Using the lm function I perform the linear regression. 

```{r}
lmodel <- lm(complete_data2$Deaths ~ ., data = ols.data)
```

```{r}
summary(lmodel)
```


### K-mean

```{r}
complete_data2 %>% recipe(~.) %>% 
  #step_mutate_at(-Age, fn = ~ as.factor(.)) %>% 
  step_dummy(all_nominal(), one_hot = T) %>% 
  step_normalize(all_predictors()) %>%
  step_nzv(all_predictors()) %>% 
  step_corr(all_predictors()) %>% 
  prep() #%>% 
```

The classification of observations into groups requires some methods for computing the distance or the (dis)similarity between each pair of observations. 
The choice of distance measures is a critical step in clustering. It defines how the similarity of two elements (x, y) is calculated and it will influence the shape of the clusters. The choice also has has a strong influence on the clustering results.

the default distance measure is the Euclidean distance. However, depending on the type of the data and the research questions, other dissimilarity measures might be preferred and you should be aware of the options.

Within R it is simple to compute and visualize the distance matrix using the functions get_dist and fviz_dist from the factoextra R package.

```{r}
distance <- get_dist(complete_data2, method = "spearman")
#head(as.matrix(distance), 2)[, 1:6]
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```
```{r}
df <- complete_data2

k2 <- kmeans(df, centers = 2, nstart = 25)
k3 <- kmeans(df, centers = 3, nstart = 25)
k4 <- kmeans(df, centers = 4, nstart = 25)
k5 <- kmeans(df, centers = 5, nstart = 25)
k6 <- kmeans(df, centers = 6, nstart = 25)
k7 <- kmeans(df, centers = 7, nstart = 25)
k8 <- kmeans(df, centers = 8, nstart = 25)
k9 <- kmeans(df, centers = 9, nstart = 25)

str(k2)
```
```{r}
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")
p5 <- fviz_cluster(k6, geom = "point",  data = df) + ggtitle("k = 6")
p6 <- fviz_cluster(k7, geom = "point",  data = df) + ggtitle("k = 6")
p7 <- fviz_cluster(k8, geom = "point",  data = df) + ggtitle("k = 6")
p8 <- fviz_cluster(k9, geom = "point",  data = df) + ggtitle("k = 6")

library(gridExtra)
grid.arrange(p1, p2, p3, p4,p5,p6,p7,p8, nrow = 4)
```

In order to determin the optimal clusters, we can use one of the following method which includes:

- Elbow method
- Silhouette method
- Gap statisti


But here we see overlapping between cluster which we can't explain


### Trees

Decision trees do not need a normalized and scaled data set in order to build a model. Scaling the data can affect any nonlinear relationships in the data. Decision tree model are robust enough to not need normalization and can reflect any nonlinear relationships in the model that is created.


#### Random Forest

Building the random forest model

```{r}
randoModel <- randomForest(X.train, Y.train, importance = TRUE, ntree = 1000)
```

Creating a prediction using the previously created model.

```{r}
set.seed(150)
randomPred <- predict(randoModel, newdata = X.test)
randomResample <- postResample(pred=randomPred, obs = Y.test)
```

Below is a plot of the top ten important variables for the Random Forest model. The parameter "Eggs" is important for both node purity and mean standard error. The other important variables are `Oilcrops`, `Alcholic.Beverages`, `Vegetal.Products`, and `Animal.Products`. Both vegetable and animal products include raw, cooked, and processed products. The inclusion of vegetable and animal products is not surprising since that is a common and somewhat necessary staple of any human diet. What is most surprising is Eggs, since it is a common food item but also a known allergen. 

```{r}
varImpPlot(randoModel, n.var=10)
```

#### Cubist

The Cubist model is a rule based algorithm that uses regression at the end of the tree to determine the outcome.

```{r}
set.seed(100)
cubeModel <- train(X.train, Y.train,
    method = "cubist",
    verbose = FALSE)
```

Plotting the most important features according to the cubist model, once again we see the inclusion of `Eggs` as the most important predictor. After `Obesity` and `Population`, the model determines `Alcoholic.Beverages` and `Milk...Excluding.Butter` as the other important food predictors.  

```{r}
plot(varImp(cubeModel), n=10)
```

```{r}
cubistPred <-predict(cubeModel, newdata=X.test)
cubistSample <- postResample(pred=cubistPred, obs = Y.test)
```

#### Gradient Boosted

Gradient Boosted Machine builds multiple decision trees, one at a time, each new tree helps to correct errors made by the previous tree. In theory, the gradient boosted model should produce a more accurate model than the random forest. 

```{r}
gbmGrid <- expand.grid(
         interaction.depth = seq(1, 7, by = 2),
         n.trees = seq(100, 1000, by = 50),
         shrinkage = c(0.01, 0.1),
         n.minobsinnode = 10
         )

set.seed(100)

gbmModel <- train(X.train, Y.train,
    method = "gbm",
    tuneGrid = gbmGrid,
    verbose = FALSE)
```

View the most important predictors determined by the gradient boosted model. Much like the other tree based models, we see the importance of `Eggs`,`Alcoholic.Beverages`, and `Oilcrops`. 

```{r}
summary(gbmModel)
```

```{r}
gbmPred <-predict(gbmModel, newdata=X.test)
gbmResample <- postResample(pred=gbmPred, obs=Y.test)
```


Prediction Results:

```{r}

display <- rbind(
  "Random Forest" = randomResample,
  "Gradient Boosted Tree" = gbmResample,
  "Cubist" = cubistSample)


display %>% kable() %>% kable_paper()
```
The best model using Rsquare and RMSE as the primary selection criteria, random forest produces the optimal model. The Gradient Boosted tree performed only marginally worse, but Rsquare parameter still fell below the .50
threshold. However, when it boils down to determining the most influential food predictor in deaths relating to COVID-19, it appears to be `Eggs`, according to the models. 

### Non-Linear

Due to the nonlinear nature of the target variable and the predictors, it is important to explore other nonlinear models such as support vector machine. SVM is typically used for classification but it can be used for regression. The algorithm finds a function that creates a hyperplane that separates the space between the target classes. In regards to regression, the principal is similar, finding a good fitting hyperplane. 

Since our target variable `Deaths` is numerical, we will have to perform support vector regression. 

#### Support Vector Regression

Use the svm function with a radial kernal to build a model. We're going to use the normalized training and testing set. 

```{r}
library(e1071)
svr_model <- svm(X.train.cs,
                   Y.train.cs,
                   kernal = "Radial",
                   cost=10)
  
```


```{r}
print(svr_model)
```

```{r, echo=FALSE}
pred = predict(svr_model, X.test.cs)

x = 1:length(Y.test.cs)
```

Plot of Actual Points vs Line of Predicted Points

```{r}
plot(x, Y.test.cs, pch=18, col="red")
lines(x, pred, lwd="1", col="blue")
```

```{r}
### Check Support Vector Regression Accuracy 
library(MLmetrics)
mse = MSE(Y.test.cs, pred)
mae = MAE(Y.test.cs, pred)
rmse = RMSE(Y.test.cs, pred)

r2 = R2(Y.test.cs, pred, form = "traditional")

cat(" MAE:", mae, "\n", "MSE", mse, "\n",
     "RMSE:", rmse, "\n", "R-Squared:", r2)
```

The basic support vector regression model produces an R-Square of 0.4499807 and an RMSE of 0.5300456. The plot shows that the model produces a poor fit. We can tune the model with different values of epsilon and cost to create a better model. 

Tune SVM:

The tune function identifies the best parameters by performing a grid search.

```{r}
tuneResult <- tune(svm, Deaths~., data=nrmData, ranges = list(epsilon=seq(0,1,0.1), cost = 2^(seq(0.5,8,.5))))
```

Here is the plot of the tuned model. The darker shades of the plot show the optimal parameters.

```{r}
plot(tuneResult)
print(tuneResult)
```


```{r}
tunedVals <- tuneResult$best.model
predict2 <- predict(tunedVals, X.test.cs)
```

The updated plot of the tuned model. It shows a better fit than the non-tuned model.

```{r}
x1 = 1:length(Y.test.cs)

plot(x1, Y.test.cs, pch=18, col="red")
lines(x1, predict2, lwd="1", col="blue")

```

Checking the accuracy of the tuned model:

```{r}
mse = MSE(Y.test.cs, predict2)
mae = MAE(Y.test.cs, predict2)
rmse = RMSE(Y.test.cs, predict2)

r2 = R2(Y.test.cs, predict2, form = "traditional")

cat(" MAE:", mae, "\n", "MSE", mse, "\n",
     "RMSE:", rmse, "\n", "R-Squared:", r2)

```

The tuned model's RSquare is much higher at 0.7057181 and a better RMSE score oef 0.3462124. We can manually check the tuned model's most important features:

```{r}

w <- t(tunedVals$coefs) %*% tunedVals$SV

w <- apply(w, 2, function(v){sqrt(sum(v^2))})

w <- sort(w, decreasing = T)

print(w)
```

The tuned SVR model places a high level of importance on `Alcoholic.Beverages`, `Animal.fats`, `Obesity`, `Stimulants`, and `Oilcrops` when predicting deaths related to COVID-19. This makes logical sense since these factors are known (when consumed in large amounts) to be associated with overall poor health. 


## Conclusion & Findings

In this analysis, we used several models to study the correlation between countries' diet and COVID-19 mortality rate.

By exploring each of those methods (which we include details in related section), we found out that food cultures is very important key as in predicting the mortality rate of COVID-19. 

The models show foods associated with overall poor health increases the mortality rate of COVID-19 in a given country. That's it, a population which has a healthy diet (food based on vegetal products, cereals,..) has a lower death rate in comparison with a population which has a higher obesity rate and consumes more animal products. In other words, a population with a healthy diet has a low rate of deaths related to the COVID-19.



**Sources:**

https://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf
https://alex.smola.org/papers/2004/SmoSch04.pdf
https://stackoverflow.com/questions/34781495/how-to-find-important-factors-in-support-vector-machine

