---
title: "Final Project"
author: "Abdellah AitElmouden | Gabriel Abreu |  Jered Ataky"
date: "12/1/2021"
output:
  html_document: default
  pdf_document: default
---


```{r}
library(GGally)
library(ggplot2)
library(caret)
library(mice)
library(tidyverse)
library(DataExplorer)
library(MASS)
library(naniar)
library(kableExtra)
library(skimr)
library(randomForest)
library(fitdistrplus)
library(corrplot)
```


## Introduction

Nutritional factors play a key role in both innate and adaptive immunity. Further, we have learnt that individuals with comorbidities are disproportionally affected with severe COVID-19 disease and mortality. Obesity, type 2 diabetes, atherosclerotic cardiovascular disease, and hypertension are risk factors for severe COVID-19.5 6 The aetiology of these conditions is largely driven by poor nutrition and unfavourable lifestyle choices (eg, physical inactivity or sedentary behaviour) 

The goal of this analysis is to find out how a country’s diet correlates with its COVID-19 mortality rate. With different food cultures across the world, it would be interesting to see what are the food categories that can best predict a country’s rate of deaths.

## The Dataset

Data for this project was taken from this very interesting [kaggle](https://www.kaggle.com/mariaren/covid19-healthy-diet-dataset) dataset. the dataset combined data of different types of food, world population obesity and undernourished rate, and global COVID-19 cases count from around the world in order to learn more about how a healthy eating style could help combat the Corona Virus.

There are 5 files in the dataset:

- Fat_Supply_Quantity_Data.csv: percentage of fat intake from different food groups for 170 different countries.
- Food_Supply_Quantity_kg_Data.csv: percentage of food intake( in  kg  ) from different food groups for 170 different countries.
- Food_Supply_kcal_Data.csv: percentage of energy intake (in  kcal ) from different food groups for 170 different countries.
- Protein_Supply_Quantity_Data.csv: percentage of protein intake from different food groups for 170 different countries.
All of these files have, also, columns including obesity, undernourishment and COVID-19 cases as percentages of total population.
- Supply_Food_Data_Descriptions.csv: This dataset is obtained from FAO.org, and is used to show the specific types of food that belongs to each category for the above datasets.

In each of the 4 datasets above, they have calculated fat quantity, energy intake (kcal), food supply quantity (kg), and protein for different categories of food (all calculated as percentage of total intake amount). they've also added on the obesity and undernourished rate (also in percentage) for comparison. The end of the datasets also included the most up to date confirmed/deaths/recovered/active cases (also in percentage of current population for each country).

**Data Collection**

- Data for different food group supply quantities, nutrition values, obesity, and undernourished percentages were obtained from Food and Agriculture Organization of the United Nations [FAO website](http://www.fao.org/faostat/en/#home) To see the specific types of food included in each category from the FAO data.
- Data for population count for each country comes from Population Reference Bureau [PRB website](https://www.prb.org/).
- Data for COVID-19 confirmed, deaths, recovered and active cases are obtained from Johns Hopkins Center for Systems Science and Engineering [CSSE website](https://coronavirus.jhu.edu/map.html).
- The USDA Center for Nutrition Policy and Promotion diet intake guideline information can be found in [ChooseMyPlate.gov](https://www.choosemyplate.gov/).


## Methodology

Before applying machine learning models, We started first with the process to understand the data which is called Exploratory Data Anaysis(EDA). It refers to the process of initial investigation and analysis of the dataset in order to understand the distribution, anomality, correlation and other data characteristics .

In this project we especially focus on using tree-based algorithms such as random forest, gradient boosting and Cubist. Decision tree analysis it is a general, predictive modelling tool, and It is one of the most widely used and practical methods for supervised learning. our goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. After that we'll proceed with a Random forests analysis, it is commonly reported as the most accurate learning algorithm. and also because it reduces the variance seen in decision trees. Also to check for more comprehensive predictive accuracy we used a cubist algorithm and compare all three models results to demonstrate which one has the highest prediction
performance

## Data Exploration and Processing

#### The whole data overview

```{r setup, include=FALSE}
fat_data <- read.csv("./data/Fat_Supply_Quantity_Data.csv")
supply_kcal_data <- read.csv("./data/Food_Supply_kcal_Data.csv")
supply_kg_data <- read.csv("./data/Food_Supply_Quantity_kg_Data.csv")
protein_data <- read.csv("./data/Protein_Supply_Quantity_Data.csv")
```

After importing the files we have noticed that the datasets are similar, and we choose to focus on the easiest to understand: food_kcal. Caloric intake from specific food groups makes it easy to understand which food groups contribute the most to a country's diet and overall calorie count. Diets can be high caloric but traditional thinking dictates that diets rich in vegetables and fruits will be healthier than those that eat a large amount of meat and sugars. Also the Unit column only contains the % sign, indicating that all the column except the Population one are in percentages. It is important to know the unit used.

the data are organized by countries. There are 170 countries in these datasets. 32 variables as following:

```{r}
names(supply_kcal_data)
```
### Check for missing Data

Missing data in substantive research is common and can be problematic for structural equation modelling if not handled correctly. A well recognised statistical benchmark suggests datasets with missingness less than 5% is not a problem. Within this dataset we checked if our data variable had less than 5% missing data and therefore not be considered a problem to conduct further statistical tests. quick screening the data we can see that certain columns have some null values. The plot below generated using the R plot_missing function helps us understand that almost 4% of data are missing in the "Recovered", "Deaths", "Confirmed", ""Undernourished" and "Active" Variables. and almost 2% in the "Obesity", So still can imput the missing values using predictive mean matching method, that is a widely used statistical imputation method for missing values, so can get a better predictive accuracy.

```{r}
plot_missing(supply_kcal_data,  missing_only = TRUE)
```
**Imputation**

```{r, echo=FALSE}
#Giving the countries with an undernourished percentage of <2.5, a flat 2. This eliminates the less than character and doesn't rely on 
#imputation to make up a new number. The imputed data would give a country a like the United States a percentage of 47.5, which can greatly
#affect how it is clustered
raw_data <- supply_kcal_data
raw_data$Undernourished[raw_data$Undernourished == "<2.5"] <- "2"

#Make the Undernourished column numeric
raw_data$Undernourished <- as.numeric(raw_data$Undernourished)
```

```{r, echo=FALSE}
#Use mice package to impute the missing data
imputed_data <- mice(raw_data, m=2, maxit = 2, method='cart', seed = 500)
complete_data <- complete(imputed_data, 2)
```

```{r, echo=FALSE}
complete_data2 <- complete_data %>% dplyr::select(c(-"Country",-"Unit..all.except.Population.", -"Confirmed",-"Active"))
```


### Statistical Analyses

**Numerical Variables Distribution**

The statistical analysis included a prior investigation on the distribution of the independent and dependent variable (recovery from COVID-19), testing different types of probability distribution. 

First we will quickly display a broad overview of a data frame, using the skim function

```{r}
skim(complete_data2)
```

from the histogram plot below we can see that the majority of the predictors are skewed and all of the values for `Aquatic.Products..Other` equals 0, making it a prime candidate to eliminate from further analysis. This data set will benefit from a Box Cox transformation. 


```{r}
plot_histogram(complete_data2, ggtheme = theme_linedraw())
```

Some other graphical methods, maybe more helpful than the simple histogram. we used the fitdisrplus package in R to visualize the recovery variable data together with some possible theoretical distributions in a skewness-kurtosis space:

```{r}
plotdist(complete_data2$Re, histo = TRUE, demp = TRUE)
```
From the empirical density above, our distribution is right skewed and appears to be an exponential type of distribution. The Cullen and Frey Graph below is a good way to exempt some distributions by the parameters of skewness and kurtosis using the descdist function; The orange values around the blue (data) point are based on bootstrapping. From this Cullen and Frey Graph and the empirical graphs above, our choices for good fits would seem to be limited to the available distributions in the fitdistrplus package:

- Weibull
- gamma
- exponential


```{r}
library(fitdistrplus)
descdist(complete_data2$Recovered, boot=1000) 
```
```{r}

# I added the > 0 because the gamma dist doesn't allow zero values 
fw <- fitdist(complete_data2$Recovered[complete_data2$Recovered > 0], distr = "weibull", lower = 0.0)
fg <- fitdist(complete_data2$Recovered[complete_data2$Recovered > 0], distr = "gamma", lower = 0.0)
fe <- fitdist(complete_data2$Recovered[complete_data2$Recovered > 0], distr = "exp", lower = 0.0)
par(mfrow = c(2, 2))
plot.legend <- c("Weibull", "gamma", "expo")
denscomp(list(fw, fg, fe), legendtext = plot.legend)
qqcomp(list(fw, fg, fe), legendtext = plot.legend)
cdfcomp(list(fw, fg, fe), legendtext = plot.legend)
ppcomp(list(fw, fg, fe), legendtext = plot.legend)
```

From the plotted fitting metrics above, it appears that Weibull and gamma are the best contenders, but it seems a little unclear who is the clear winner form the density plot. Let’s take a closer look with a larger density plot for these two distributions.

```{r}
denscomp(list(fw, fg), legendtext = c("Weibull", "gamma"))
```
It seems that still both distribution fits this data the best. Let us confirm this against the Akaline’s and Bayesian Information Criterion (AIC and BIC), which will give a sort of rank to the goodness of fit models passed to it using the gofstat function as well as the Goodness-of-fit statistics, which give distances between the fits and the empirical data.

```{r}
gofstat(list(fw, fg))
```
Since the gamma distribution has the min AIC, BIC, and minimum goodness-of-fit statistics, we will choose the gamma distribution.




```{r, echo=FALSE}
#Select top 20 countries of different predictors (those usually associated with healthy or unhealthy diets) 
top_obese_countries <- complete_data %>% 
                       dplyr::select(Country, Obesity) %>%
                       arrange(desc(Obesity)) %>%
                       head(20)


top_deaths_countries <- complete_data %>% 
                        dplyr::select(Country, Deaths) %>%
                        arrange(desc(Deaths)) %>%
                        head(20)

top_veggies_countries <- complete_data %>% 
                        dplyr::select(Country, Vegetables) %>%
                        arrange(desc(Vegetables)) %>%
                        head(20)

top_meat_countries <- complete_data %>% 
                        dplyr::select(Country, Meat) %>%
                        arrange(desc(Meat)) %>%
                        head(20)

top_aniproducts_countries <- complete_data %>% 
                        dplyr::select(Country, Animal.Products) %>%
                        arrange(desc(Animal.Products)) %>%
                        head(20)

top_sugar_countries <- complete_data %>% 
                        dplyr::select(Country, Sugar...Sweeteners) %>%
                        arrange(desc(Sugar...Sweeteners)) %>%
                        head(20)
```


```{r}
par(mfrow=c(2,2))
ggplot(top_obese_countries, aes(x=Obesity, y=Country)) + geom_histogram(stat = "identity")
ggplot(top_deaths_countries, aes(x=Deaths, y=Country)) + geom_histogram(stat = "identity")
ggplot(top_veggies_countries, aes(x=Vegetables, y=Country)) + geom_histogram(stat = "identity")

```

```{r}
par(mfrow=c(2,2))
ggplot(top_aniproducts_countries, aes(x=Animal.Products, y=Country)) + geom_histogram(stat = "identity")
ggplot(top_meat_countries, aes(x=Meat, y=Country)) + geom_histogram(stat = "identity")
ggplot(top_sugar_countries, aes(x=Sugar...Sweeteners, y=Country)) + geom_histogram(stat = "identity")
```

```{r}

```

```{r}

```
```{r}

```

```{r}

```


## Data Manipulation
```{r, echo=FALSE}
#make a copy of the original data set 
raw_data <- supply_kcal_data
```


### Create Training/Test Set

```{r}
#Creating a non-normalized and scaled data set (only BoxCox transformation, eliminate near zero variance and highly correlated variables)
Processed <- preProcess(complete_data2, method = c("BoxCox", "nzv","corr"))
Processed
```

```{r}
treeData <- predict(Processed, complete_data2)
```

```{r}
set.seed(150)
predictors <- subset(treeData, select = -Deaths)
Deaths <- subset(treeData, select="Deaths")
initsplit <- createDataPartition(Deaths$Deaths, p=0.8, list=FALSE)

#Create Training Data to tune the model
X.train <- predictors[initsplit,]
Y.train <- Deaths[initsplit,]

#Create testing data to evaluate the model
X.test <- predictors[-initsplit,]
Y.test <- Deaths[-initsplit,]
```

```{r}
#Creating a normalized, scaled data set along with the other transformations
Processed2 <- preProcess(complete_data2, method = c("center", "scale","BoxCox","nzv","corr"))
Processed2
```

```{r}
nrmData <- predict(Processed2, complete_data2)
```

```{r}
set.seed(150)

predictors2 <- subset(nrmData, select = -Deaths)
Deaths2 <- subset(nrmData, select="Deaths")
initsplit2 <- createDataPartition(Deaths2$Deaths, p=0.8, list=FALSE)

#Create Training Data to tune the model
X.train.cs <- predictors2[initsplit2,]
Y.train.cs <- Deaths2[initsplit2,]

#Create testing data to evaluate the model
X.test.cs <- predictors2[-initsplit2,]
Y.test.cs <- Deaths2[-initsplit2,]
```
## Models


### PCA 

#### Variable correlation

Before we start our PCA analysis we selected the variables that we are interested on and checked if a significant correlation among some of these variables. We want to create a list of the strongest correlations with diets for 2 covid states: deaths, and recovered. We will not use the active, and confirmed covid state.



```{r}
M<- cor(complete_data2[, c('Alcoholic.Beverages','Animal.fats', 'Eggs', 'Fish..Seafood', 'Fish..Seafood', 'Fruits...Excluding.Wine','Meat', 'Milk...Excluding.Butter', 'Starchy.Roots','Sugar...Sweeteners','Vegetal.Products','Vegetable.Oils','Vegetables','Recovered', 'Deaths')])

#M <- cor.test(eng_imputed)

head(round(M,2))
```
We also added significance test to the correlogram we shall compute the p-value. In the correlogram blow If the p-value is greater than 0.01 then it is an insignificant value for which the cells are either blank or crossed. 


```{r}
library(RColorBrewer)
cor.mtest <- function(mat, ...) 
{
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) 
  {
    for (j in (i + 1):n)
    {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}

p.mat <- cor.mtest(complete_data2[, c('Alcoholic.Beverages','Animal.fats', 'Eggs', 'Fish..Seafood', 'Fish..Seafood', 'Fruits...Excluding.Wine','Meat', 'Milk...Excluding.Butter', 'Starchy.Roots','Sugar...Sweeteners','Vegetal.Products','Vegetable.Oils','Vegetables','Recovered', 'Deaths')])

# Correlation 
M <- cor(complete_data2[, c('Alcoholic.Beverages','Animal.fats', 'Eggs', 'Fish..Seafood', 'Fish..Seafood', 'Fruits...Excluding.Wine','Meat', 'Milk...Excluding.Butter', 'Starchy.Roots','Sugar...Sweeteners','Vegetal.Products','Vegetable.Oils','Vegetables','Recovered', 'Deaths')])


# Specialized the insignificant value
# according to the significant level
corrplot(M, method="number", type = "upper", order = "hclust", 
         p.mat = p.mat, sig.level = 0.01, tl.col = "black")

```
From the plot we can see that when we compare death and recovered cases, the animal fat, animal products significantly higher in terms of correlation. Recovered cases diets have a lesser correlation to meat diet.

####  PCA Summary

```{r}
Data.pca1 <- prcomp(complete_data2, center=TRUE, scale.=TRUE)
summary(Data.pca1)
```
**Z=XV both give the same output**

```{r}
Data.pca1$x [,1:10] %>% head(1) 
```

**V matrix - Eigenvectors**

```{r}
head(Data.pca1$rotation)
```
**Principal components**

Since the principal components are orthogonal, there is no correlation whatsoever. The correlation plot is perfectly white, apart from autocorrelation.

```{r}
res1 <- cor(Data.pca1$x, method="pearson")
corrplot::corrplot(res1, method= "color", order = "hclust", tl.pos = 'n')
```
#### PCA analysis

The plot below shows what percent of variance has been explained for each number of principal components (aggregate variance explained).

```{r}
plot(summary(Data.pca1)$importance[3,])
```
#### Plots

**PC1-PC2**

```{r}
library(factoextra)
fviz_pca_var(Data.pca1,axes = c(1, 2))
```

**PC3-PC4**

```{r}
fviz_pca_var(Data.pca1,axes = c(3, 4))
```

**PC5-PC6**

```{r}
fviz_pca_var(Data.pca1,axes = c(5, 6))
```


**PC7-PC8**

```{r}
fviz_pca_var(Data.pca1,axes = c(7, 8))
```
##### Linear regression with principal components

Below are scatterplots showing the relation between PCs and y. Some of the principal components have a high correlation and others a lower correlation with the observed dependent variable.

**Scatterplots**

```{r}
par(mfrow=c(2,2))
pcs <- as.data.frame(Data.pca1$x)
plot(complete_data2$Deaths, pcs$PC1)
plot(complete_data2$Deaths, pcs$PC2)
plot(complete_data2$Deaths, pcs$PC3)
plot(complete_data2$Deaths, pcs$PC4)
```
#### PCR

In order to perform the regression, I am combining both the explanatory variables - PCs, and explained variable - y. In this moment the dimensionality reduction should take place. Here however I won’t be getting rid of any principal components since I want to compare the results with prc from pls package.

```{r}
ols.data <- cbind(complete_data2$Deaths, pcs)
```

Using the lm function I perform the linear regression. 

```{r}
lmodel <- lm(complete_data2$Deaths ~ ., data = ols.data)
```

```{r}
summary(lmodel)
```


### Trees

Decision trees do not need a normalized and scaled data set in order to build a model. Scaling the data can affect any nonlinear relationships in the data. Decision tree model are robust enough to not need normalization and can reflect any nonlinear relationships in the model that is created.


#### Random Forest

Building the random forest model

```{r}
randoModel <- randomForest(X.train, Y.train, importance = TRUE, ntree = 1000)
```

Creating a prediction using the previously created model.

```{r}
set.seed(150)
randomPred <- predict(randoModel, newdata = X.test)
randomResample <- postResample(pred=randomPred, obs = Y.test)
```

Below is a plot of the top ten important variables for the Random Forest model. The parameter "Eggs" is important for both node purity and mean standard error. The other important variables are `Oilcrops`, `Alcholic.Beverages`, `Vegetal.Products`, and `Animal.Products`. Both vegetable and animal products include raw, cooked, and processed products. The inclusion of vegetable and animal products is not surprising since that is a common and somewhat necessary staple of any human diet. What is most surprising is Eggs, since it is a common food item but also a known allergen. 

```{r}
varImpPlot(randoModel, n.var=10)
```

#### Cubist

The Cubist model is a rule based algorithm that uses regression at the end of the tree to determine the outcome.

```{r}
set.seed(100)
cubeModel <- train(X.train, Y.train,
    method = "cubist",
    verbose = FALSE)
```

Plotting the most important features according to the cubist model, once again we see the inclusion of `Eggs` as the most important predictor. After `Obesity` and `Population`, the model determines `Alcoholic.Beverages` and `Milk...Excluding.Butter` as the other important food predictors.  

```{r}
plot(varImp(cubeModel), n=10)
```

```{r}
cubistPred <-predict(cubeModel, newdata=X.test)
cubistSample <- postResample(pred=cubistPred, obs = Y.test)
```

#### Gradient Boosted

Gradient Boosted Machine builds multiple decision trees, one at a time, each new tree helps to correct errors made by the previous tree. In theory, the gradient boosted model should produce a more accurate model than the random forest. 

```{r}
gbmGrid <- expand.grid(
         interaction.depth = seq(1, 7, by = 2),
         n.trees = seq(100, 1000, by = 50),
         shrinkage = c(0.01, 0.1),
         n.minobsinnode = 10
         )

set.seed(100)

gbmModel <- train(X.train, Y.train,
    method = "gbm",
    tuneGrid = gbmGrid,
    verbose = FALSE)
```

View the most important predictors determined by the gradient boosted model. Much like the other tree based models, we see the importance of `Eggs`,`Alcoholic.Beverages`, and `Oilcrops`. 

```{r}
summary(gbmModel)
```

```{r}
gbmPred <-predict(gbmModel, newdata=X.test)
gbmResample <- postResample(pred=gbmPred, obs=Y.test)
```


Prediction Results:

```{r}

display <- rbind(
  "Random Forest" = randomResample,
  "Gradient Boosted Tree" = gbmResample,
  "Cubist" = cubistSample)


display %>% kable() %>% kable_paper()
```
The best model using Rsquare and RMSE as the primary selection criteria, random forest produces the optimal model. The Gradient Boosted tree performed only marginally worse, but Rsquare parameter still fell below the .50
threshold. However, when it boils down to determining the most influential food predictor in deaths relating to COVID-19, it appears to be `Eggs`, according to the models. 

### Non-Linear

#### SVM


#### KNN


### Clustering(?)



