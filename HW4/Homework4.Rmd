---
title: "HMK4 | ADHD Prediction"
author: "Abdellah Ait Elmouden | Gabriel Abreu |  Jered Ataky"
date: "11/10/2021"
output: html_document
---


```{r setup, include=FALSE}
library(GGally)
library(caret)
library(mice)
library(plyr)
library(tidyverse)
library(DataExplorer)
library(MASS)
library(naniar)
library(corrplot)
library(VIM)
library(cluster)
library(recipes)
library(factoextra)
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

ML techniques can potentially offer new routes for learning patterns of human behavior; identifying mental health symptoms and risk factors; developing predictions about disease progression; and personalizing and optimizing therapies. In this project we will work with mental health dataset and use some of the unsupervised learning methods to cluster data to provide new insights, and to discover patterns and help structure the data.


## Problem Statement 

For this assignment, we will be working with a very interesting mental health dataset from a real-life
research project. All identifying information, of course, has been removed. The attached spreadsheet
has the data (the tab name “Data”). The data dictionary is given in the second tab. You can get as
creative as you want. The assignment is designed to really get you to think about how you could use
different methods.


### Exploratory Data Analysis

Exploration of the data is always the first step in data analysis. The main priorities of exploration is exploring data types, outliers, overall distribution of the data points, and missing data. We're also going to see if any transformation is possible. 


In this process we'll analye and visualize the data to get a better understanding of the data and glean insight from it. The process will involves the following steps:

- Import the data
- Clean the data
- Process the data
- Visualize the data
=======

```{r}
#Load the data
raw_adhd <- readxl::read_xlsx('ADHD_data.xlsx')
glimpse(raw_adhd)
```

The data set is mostly compromised of "dbl" type, even though some of the columns such as Sex and Race are prime candidates to be converted to factors. There are unique values for 'Age' and 'Initial'. Since the column 'Initial' is a sort of identifier, we can safely remove it, the column is not necessary for analysis.

It's important to examine the data dictionary and see which columns might be combined or separated into different data sets.

The original dataframe contains 175 observations (i.e. survey participants) as rows and 54 columns as variables.
The columns contain both qualitative and quantitative variables.
Moreover, some columns represent categorical data but is encoded as numerical values.


------------------------------------------------------------------------------------------------------------------------------
Data Dictionary:
Sex: Male - 1, Female - 2
Race: Race:  White-1, African American-2, Hispanic-3, Asian-4, Native American-5, Other or missing data -6
ADHD self-report scale: Never-0, rarely-1, sometimes-2, often-3, very often-4
Mood disorder questions: No-0, yes-1; question 3: no problem-0, minor-1, moderate-2, serious-3
Individual substances misuse:  no use-0, use-1, abuse-2, dependence-3
Court Order:  No-0, Yes-1
Education: 1-12 grade, 13+ college
History of Violence: No-0, Yes-1
Disorderly Conduct: No-0, Yes-1
Suicide attempt: No-0, Yes-1
Abuse Hx: No-0, Physical (P)-1, Sexual (S)-2, Emotional (E)-3, P&S-4, P&E-5, S&E-6, P&S&E-7
Non-substance-related Dx: 0 – none; 1 – one; 2 – More than one
Substance-related Dx: 0 – none; 1 – one Substance-related; 2 – two; 3 – three or more
Psychiatric Meds: 0 – none; 1 – one psychotropic med; 2 – more than one psychotropic med
------------------------------------------------------------------------------------------------------------------------------

Identifying the missing data from the data set:

```{r}

plot_missing(
  raw_adhd,
  group = list(Good = 0.05, OK = 0.4, Bad = 0.8, Remove = 1),
  missing_only = TRUE,
  geom_label_args = list(),
  title = NULL,
  ggtheme = theme_gray(),
  theme_config = list(legend.position = c("bottom"))
)

```

There are columns missing data, with the exception of Psychiatric Meds, most of the missing data appears imputable. The 'Psychiatric Meds' column is missing 67.43% of data points. Imputing the data for this column is not optimal since most of the data would be imputed.  

```{r}
gg_miss_upset(raw_adhd)
```


```{r message=FALSE, warning=FALSE}
# rename data column names
raw_adhd <- raw_adhd %>% dplyr::rename_all(funs(make.names(.)))
```


```{r message=FALSE, warning=FALSE}
imputed_Data <- mice(raw_adhd, m=2, maxit = 2, method = 'cart', seed = 500)
```
```{r}
raw_adhd <- complete(imputed_Data,2)
```


```{r}
library(skimr)
skim(raw_adhd)
```

#### Data Cleaning 

```{r}
#Remove the Psychiatric Medicine and Initial Columns from data set.
adhd_df <- raw_adhd[-c(1, 54)]
jus_nums <- raw_adhd[-c(1, 54)]

```

#### Data Exploration

The various distribution of the responses to the ADHD questionnaire are displayed below. Also Distribution of Binary and Ordinal Variables.

```{r}
plot_histogram(adhd_df, ggtheme = theme_linedraw())

```
After observing the plots to assess how the distributions involved in the dataset. Based on the histrograms plotted above, we can note that there are many observations although numeric, behave as categorical features and this will need to be assessed when performing the kmeans clustering analysis. There does not seem to be any clear distinguishable outliers however there does seem to be some features that experience low variance such Stimulants where majority of the recorded observations are 0.


#### Data Transformation / Further Data Exploration

The columns Sex and Age are going to be converted to factors. When building future models, it's easier to read the different clusters or groups with certain categorical data turned into factors with appropriate labels. 

```{r}

adhd_df$Sex <-factor(adhd_df$Sex, levels = c(1,2), labels=c('Male','Female'))

adhd_df$Race <-factor(adhd_df$Race, levels=c(1,2,3,4,5,6), labels = c('White', 'African American', 'Hispanic', 'Asian', 'Native American', 'Other'))

adhd <- adhd_df
```

```{r}
adhd_df[sapply(adhd_df, is.double)] <- lapply(adhd_df[sapply(adhd_df, is.double)], as.factor)

```

```{r}
glimpse(adhd_df)
```

### Clustering

Clustering is the most common form of unsupervised learning. It's a machine learning algorithm used to draw inferences from unlabeled data. The algorithm groups a set of data points into subsets. The goal is to create clusters that are have minimal variance internally, but maximum variance from external clusters. 

There are methods to clustering data. One is agglomerative, each observation in a respective cluster then merging together until a stop criteria is reached. The second is divisive, with all observations in a single cluster and subsequent splitting occures until a stop criteria is met. 

#### k-means clustering

K-means clustering is one of the simplest and popular unsupervised machine learning algorithms. Typically, unsupervised algorithms make inferences from datasets using only input vectors without referring to known, or labelled, outcomes.

To perform a cluster analysis in R, generally, the data should be prepared as follows:

- Removal totalized features
- Numeric to Factor Conversions
- Newly transformed categorical variables were binarized into 0/1. k-means will not be able to distinguish the eucliiand distances properly between classes that span more than 2 categories. 

- Normalization: features need to be normalized such that the distances they are centered and scaled the mean is 0 and the Stdev is 1, this scales all the data to allow kmeans to appropriately place centroids and observations at appropriate distances.

- Colinearity test: Colinearity was tested and it was determined that there was not sufficient colinearity between any variables such that they needed to be removed for this reason alone.

- Removed low-variance features: From Data Exploration section Stimulants seems like a low-variance variable with majority of categories recorded at 0. 


```{r}
adhd_df %>% recipe(~.) %>% 
  step_rm(contains("total")) %>% 
  #step_mutate_at(-Age, fn = ~ as.factor(.)) %>% 
  step_dummy(all_nominal(), one_hot = T) %>% 
  step_normalize(all_predictors()) %>%
  step_nzv(all_predictors()) %>% 
  step_corr(all_predictors()) %>% 
  prep() #%>% 
```
The classification of observations into groups requires some methods for computing the distance or the (dis)similarity between each pair of observations. 
The choice of distance measures is a critical step in clustering. It defines how the similarity of two elements (x, y) is calculated and it will influence the shape of the clusters. The choice also has has a strong influence on the clustering results.

the default distance measure is the Euclidean distance. However, depending on the type of the data and the research questions, other dissimilarity measures might be preferred and you should be aware of the options.

Within R it is simple to compute and visualize the distance matrix using the functions get_dist and fviz_dist from the factoextra R package.

```{r}
distance <- get_dist(adhd_df)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

```


```{r}
df <- na.omit(adhd_df)
k2 <- kmeans(df, centers = 2, nstart = 25)
k3 <- kmeans(df, centers = 3, nstart = 25)
k4 <- kmeans(df, centers = 4, nstart = 25)
k5 <- kmeans(df, centers = 5, nstart = 25)
str(k2)
```
We can also view our results by using fviz_cluster. This provides a nice illustration of the clusters. If there are more than two dimensions (variables) fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance.

Because the number of clusters (k) must be set before we start the algorithm, it is often advantageous to use several different values of k and examine the differences in the results. We can execute the same process for 3, 4, and 5 clusters, and the results are shown in the figure:

```{r}
# I had to convert df back to numeric to plot the clusters

df[sapply(df, is.factor)] <- lapply(df[sapply(df, is.factor)], as.numeric)

p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

**Determining Optimal Clusters**

As you may recall the analyst specifies the number of clusters to use; preferably the analyst would like to use the optimal number of clusters. To aid the analyst, the following explains the three most popular methods for determining the optimal clusters, which includes:

- Elbow method
- Silhouette method
- Gap statistic

Recall that, the basic idea behind cluster partitioning methods, such as k-means clustering, is to define clusters such that the total intra-cluster variation (known as total within-cluster variation or total within-cluster sum of square) is minimized

The total within-cluster sum of square (wss) measures the compactness of the clustering and we want it to be as small as possible. Thus, we can use the following algorithm to define the optimal clusters:

- Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters
- For each k, calculate the total within-cluster sum of square (wss)
- Plot the curve of wss according to the number of clusters k.
- The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.


Fortunately, this process to compute the “Elbow method” has been wrapped up in a single function (fviz_nbclust):

```{r}
set.seed(123)

fviz_nbclust(df, kmeans, method = "wss")
```
The results suggest that 4 is the optimal number of clusters as it appears to be the bend in the knee (or elbow).

**Extracting Results**

With most of these approaches suggesting 2 as the number of optimal clusters, we can perform the final analysis and extract the results using 2 clusters.

```{r}
set.seed(123)
final <- kmeans(df, 2, nstart = 25)
fviz_cluster(final, data = df)
```


#### Hierarchical Clustering

Hierarchical Clustering works by repeatingly combining the two nearest clusters into a larger cluster. It is a bottom-up approach which doesn't require the specification of the number of clusters beforehand. 

The final structure of the cluster is represented by a dendrogram diagram. 

In order to perform hierarchical clustering we will perform the following:

1. Normalize our continuous numerical values.Each observations feature values are presented as coordinates in n-dimensional space (n being the number of predictors/features). The distances between coordinates will be calculated in step 2 using a standard of measure. However, if the coordinates are normalized it may lead to false results.  

The continous variables in the data set are Age, MD.Total, and MD.Total. 


```{r}
set.seed(300)
#scale the numerical continuous data

adhd3 <- adhd_df
adhd3$Age <- as.numeric(adhd3$Age)
adhd3$MD.TOTAL <- as.numeric(adhd3$MD.TOTAL)
adhd3$ADHD.Total <- as.numeric(adhd3$ADHD.Total)
adhd3 %>% select_if(is.numeric) %>% lapply(scale) ->adhd3
adhd3 <- as.data.frame(adhd3)
adhd4 <- adhd_df
adhd4$Age <- adhd3$Age
adhd4$ADHD.Total <- adhd3$ADHD.Total
adhd4$MD.TOTAL <- adhd3$MD.TOTAL
```

2. We will use the euclidean measure of distance. The default of the dist function is euclidean which is the square distance between two vectors. 

```{r}
#compute the distance
dist_mat <- dist(adhd4, method = 'euclidean')
```

3. The linkage method selected is 'ward.D2'. The 'average' method was originally tested, but the clusters were not as complete. The ward method minimizes the total variance within the clusters. 

```{r}
#select linkage method

hclust_comp <- hclust(dist_mat, method = 'ward.D2')
plot(hclust_comp)
```

4. Find the optimal number of clusters. We're going to utilize the "NbClust" library and visualize the optimal number of clusters. In order to use the NbClust function, the numerical-only data set is necessary. 


```{r}
library(NbClust)
adhd_clust <- NbClust(
                      data = scale(jus_nums),
                      distance = 'euclidean',
                      min.nc = 2, 
                      max.nc = 10,
                      method="ward.D2", 
                      index="all")
```

I tried the scale and unscaled versions of the data set, the optimal cluster is 2. So that's going to be our k in our dendrogram. 

```{r}
#View the different clusters
plot(hclust_comp)
rect.hclust(hclust_comp , k = 2, border = 2:6)
#abline(h = 39, col = 'red')
```

```{r}
hierarchyGroups <- cutree(hclust_comp, 2)
```


```{r}
fviz_cluster(list(data = jus_nums, cluster = hierarchyGroups))
```

Looking at the cluster plots, we get two distinct clusters. The next step is identifying the two groups.

#### Identity Groups




------------------------------------------------------------------------------------------------------------------------------------------------------
#### Question 2 Principal Component Analysis (PCA)

PCA commonly used for dimensionality reduction by using each data point onto only the first few principal components (most cases first and second dimensions) to obtain lower-dimensional data while keeping as much of the data’s variation as possible.

Features that are strongly correlated with each other plotare more suitable for PCA than those loosely related. Below corrplot using the Spearman correlation for the categorical variables demonstrate that the features within ADHD set are more strongly correlated than the MD set. In addition, there are too many missing values for the individual substance misuse, and therefore PCA is not performed on this set. For question 2, we conduct PCA on both ADHD and MD, but ADHD is demonstrated to be more suitable for the task.

##### PCA - ADHD

```{r}
# pca - ADHD
PCA_adhd <- prcomp(df %>% dplyr::select(contains("adhd.")))

```

```{r}
sd <- PCA_adhd$sdev
loadings <- PCA_adhd$rotation
rownames(loadings) <- colnames(df %>% dplyr::select(contains("adhd.")))
scores <- PCA_adhd$x

summary(PCA_adhd)
```

We will visualize the results using scree and cumulative variance plot. The scree plot (“elbow” method)indicate that the first 4 components are the most important ones, although they merely captured roughly 32% of the variance in the data. The first 29 components have the standard deviation above 1 and together they captured 77% of the variance. The number of dimensions is significantly reduced from 91 to 29 in this case.

```{r}
par(mfrow = c(1, 2))

# scree plot
screeplot(PCA_adhd, type = "l", npcs = 30, main = "Screeplot of the first 30 PCs")
abline(h = 1, col = "red", lty = 5)
legend("topright", legend = c("Eigenvalue = 1"), col = c("red"), lty = 5, cex = 0.6)

# cumulative variance plot
cumpro <- cumsum(PCA_adhd$sdev^2 / sum(PCA_adhd$sdev^2))
plot(cumpro[0:30], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
```
**Loading**

Loadings are interpreted as the coefficients of the linear combination of the initial variables from which the principal components are constructed. From a numerical point of view, the loadings are equal to the coordinates of the variables divided by the square root of the eigenvalue associated with the component.

adhd_total is the most important contributor to the first principal component. PC1 is made up by majority of "_X4" variables, meaning the response of “very often” from the ADHD questions; whereas PC2 is made up by majority of "_X0" variables, meaning the response of “never” from the ADHD questions. As expected, PCA successfully extract components from features that are not strongly associated to each other.

```{r}
library(kableExtra)
cut_off <- sqrt(1/ncol(df %>% dplyr::select(contains("adhd.")))) 

loadingsDf <- loadings %>% 
    as.data.frame() %>% 
    tibble::rownames_to_column() %>%
    dplyr::select(variables = rowname, everything())

pc1_important <- loadingsDf %>% 
    dplyr::filter(abs(PC1) > cut_off) %>%
    dplyr::select(variables, PC1) %>%
    arrange(desc(abs(PC1)))

pc1_important %>% 
  kable() %>%
  scroll_box()
```

The biplot displays the individuals and variables in the same plot featuring the first two principal components. It seems to suggest that the individuals can be visually clustered into 4 groups based on their responses to the ADHD questions.


```{r}
biplot(PCA_adhd, scale = 0, cex = 0.5)
```
------------------------------------------------------------------------------------------------------------------------------------------------------
### Question 3 Gradient Boosting






------------------------------------------------------------------------------------------------------------------------------------------------------
### Question 4 Support Vector Machine

SVM (Support Vector Machines) are a supervised learning method that can be used for classification and regression analysis. Typically, there are used more for classification. The basic idea of SVM is to create an optimal hyperplane to linearly seperate variables, using support vectors. Support vectors are the data points the are closest to the hyperplane. These are the points that are most difficult to classify.  The goal of SVM is to create the best hyperplane, or one that maximizes the distance/magrin between the support vectors. SVM does work with non-linear data using kernal functions. Kernal Functions maps the data points to a higher dimension. 

For the purposes of this question, we want to use SVM to classify suicides. In order to use SVM, we're going to need to reduce the total dimensions in the data set. SVM works best with less dimensions, in order to perform dimension reduction, we're going to apply 2 approaches. One is to eliminate variables with higher than normal levels of correlation and secondly, implement PCA to create new variables that account for most of the data. 


#### SVM with Dimension Reduction using Correlation 

We're going to create a new data set and convert Suicide into a factor.

```{r}
adhd_svm_data <- adhd
adhd_svm_data$Suicide<- factor(adhd_svm_data$Suicide)
```

Selecting only the numerical values, a corr plot is developed in an effort to identify variables with high correlation values. Once they're identified, the total number of variables can be reduced. Just by isolating the numerical columns, there are 49 columns.

```{r}
adhd_svm_data %>% select_if(is.numeric) %>% cor() -> svmCor
corrplot(svmCor, method = 'circle')
```



```{r}
svmNumerical <- adhd_svm_data %>% dplyr::select( -Race, -Suicide, -Sex)
svmNumerical <- svmNumerical[,-c(nearZeroVar(svmNumerical))]
corr_data <- findCorrelation(svmCor, cutoff = 0.55)
svmNumerical2 <- svmNumerical[, -corr_data]
```

After cutting off correlation at 0.55, the number of variables left are 29 continuous variables. In total, we are moving forward with this SVM model with 33 variables.

```{r}
svmNumerical2$Race <- adhd_svm_data$Race
svmNumerical2$Suicide <- adhd_svm_data$Suicide
svmNumerical2$Sex <- adhd_svm_data$Sex
```


```{r}
set.seed(100)

#Create Data Partition 

initsplit <- createDataPartition(svmNumerical2$Suicide, p=0.8, list=FALSE)


#Create Training Data to tune the model
training <- svmNumerical2[initsplit,]


#Create testing data to evaluate the model
test <- svmNumerical2[-initsplit,]

```


```{r}

svmfit_s = svm(Suicide ~ ., data = training, kernel = "sigmoid", cost = 10, scale = TRUE)
svmfit_r = svm(Suicide ~ ., data = training, kernel = "radial", cost = 10, scale = TRUE)
svmfit_l =svm(Suicide ~ ., data = training, kernel = "linear", cost = 10, scale = TRUE)
```

```{r}
svmPre1<-predict(svmfit_s,test)
svm1<-confusionMatrix(svmPre1,test$Suicide)
svm1
```

```{r}
svmPre2<-predict(svmfit_r,test)
svm2<-confusionMatrix(svmPre2,test$Suicide)
svm2
```


```{r}
svmPre3<-predict(svmfit_l,test)
svm3<-confusionMatrix(svmPre3,test$Suicide)
svm3
```


#### SVM - Dimension Reduction with PCA 

Using the 'jus_nums' data set (all the variables are still numerical but with imputed values).

```{r}
#jus_nums2 <- jus_nums %>% dplyr::select(-MD.TOTAL, -ADHD.Total)
jus_nums2$Suicide <- as.factor(jus_nums2$Suicide)

```

```{r}
PCA_svm <- prcomp(jus_nums2 %>%dplyr :: select(-Suicide), center = TRUE, scale. = TRUE)
summary(PCA_svm)
```

```{r}
fviz_pca_biplot(PCA_svm, label="var",
             habillage = jus_nums2$Suicide,
             addEllipses = TRUE, palette = "jco")

```

When can see the the 2 most important components of the account for 34.781 % of the data. We can utilize 34-35 components, those cover a little over 95% of the data, however, the fear is to overfit the model. It is necessary to reduce the number of PCA components so we can continue building our SVM model. The 'elbow method' can be used to reduce the overall number of compnents. 

```{r}
fviz_eig(PCA_svm, addlabels = TRUE)
```

Looking at the scree plot, the optimal number of PCA components is 3, the rate of decrease slows down after 3. Using the three principal components, the next step is to create a new data set with Suicide as the fifth column. After the new data set is created, we can split the data into training and testing sets. The new data set will have 4 total columns, a large reduction in dimensions from the original data set. 

```{r}
#Select the 4 components and assign to a new variable 
pca_3 <- PCA_svm$x[,1:3]

#create new data set

pca_new <- cbind(as.data.frame(pca_4), Suicide = jus_nums2$Suicide)
```


```{r}
set.seed(100)

#Create Data Partition 

initsplit <- createDataPartition(pca_new$Suicide, p=0.8, list=FALSE)


#Create Training Data to tune the model
training2 <- pca_new[initsplit,]


#Create testing data to evaluate the model
test2 <- pca_new[-initsplit,]


```

We're going to build several models using different kernals. 

```{r}
svmfit2 = svm(Suicide ~ ., data = training2, kernel = "radial", cost = 10, scale = FALSE)
svmfit3 = svm(Suicide ~ ., data = training2, kernel = "linear", cost = 10, scale = FALSE)
svmfit4 = svm(Suicide ~ ., data = training2, kernel = "sigmoid", cost = 10, scale = FALSE)
```

Radial Kernal Results

```{r}
svmPre2<-predict(svmfit2,test2)
svm2<-confusionMatrix(svmPre2,test$Suicide)
svm2
```
Linear Kernal Results

```{r}
svmPre3<-predict(svmfit3,test2)
svm3<-confusionMatrix(svmPre3,test$Suicide)
svm3
```
Sigmoid Kernal Results

```{r}
svmPre4<-predict(svmfit4,test2)
svm4<-confusionMatrix(svmPre4,test$Suicide)
svm4
```

#### Selecting a final model

Interestingly, three of the six models results in the same level of accuracy with the testing data. The model using the sigmoid kernal with reduced variables due to correlation was the best out of that approach. On the other hand, the linear and radial kernal returned similar results using PCA to trim the overall variables. Now, it is important to look into the sensitivity and specifity of the top models. The specificity of 2 of the top models was 50% and 40% respectively, while the third model had a specificity of 0%. This is really important because of the topic and our end goals. The goal is to detect suicide in patients. 1 of the models can predict suicide at the rate of a coin toss and the second less than that, which is terrible. I would rather go with the third model that has no specificity. The model with a 100% sensitivity can at least RULE out non-suicidal people. In a real world enviroment, it's better to be wrong about who you think is suicidal then accidental classify someone as non-suicidal who actually is suicidal. Also, the PCA models contain a much reduced dimesionality, making them easier models. In a professional setting, the downside of such models would be explaining it to a non-technical audience. 



Sources:

- https://towardsdatascience.com/linear-discriminant-analysis-lda-101-using-r-6a97217a55a6
- https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff
- https://towardsdatascience.com/4-ways-to-reduce-dimensionality-of-data-8f82e6565a07
- https://www.datacamp.com/community/tutorials/hierarchical-clustering-R#what
- Kmean clustring https://uc-r.github.io/kmeans_clustering
- gradient boosting machine in R : https://www.storybench.org/tidytuesday-bike-rentals-part-2-modeling-with-gradient-boosting-machine/






