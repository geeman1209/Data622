---
title: "HMK4 | ADHD Prediction"
author: "Abdellah Ait Elmouden | Gabriel Abreu |  Jered Ataky"
date: "11/10/2021"
output: html_document
---


```{r setup, include=FALSE}
library(GGally)
library(caret)
library(mice)
library(plyr)
library(tidyverse)
library(DataExplorer)
library(MASS)
library(naniar)
library(corrplot)
library(VIM)
library(cluster)
library(recipes)
library(factoextra)
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

ML techniques can potentially offer new routes for learning patterns of human behavior; identifying mental health symptoms and risk factors; developing predictions about disease progression; and personalizing and optimizing therapies. In this project we will work with mental health dataset and use some of the unsupervised learning methods to cluster data to provide new insights, and to discover patterns and help structure the data.


## Problem Statement 

For this assignment, we will be working with a very interesting mental health dataset from a real-life
research project. All identifying information, of course, has been removed. The attached spreadsheet
has the data (the tab name “Data”). The data dictionary is given in the second tab. You can get as
creative as you want. The assignment is designed to really get you to think about how you could use
different methods.


### Exploratory Data Analysis

Exploration of the data is always the first step in data analysis. The main priorities of exploration is exploring data types, outliers, overall distribution of the data points, and missing data. We're also going to see if any transformation is possible. 


In this process we'll analye and visualize the data to get a better understanding of the data and glean insight from it. The process will involves the following steps:

- Import the data
- Clean the data
- Process the data
- Visualize the data
=======

```{r}
#Load the data
raw_adhd <- readxl::read_xlsx('ADHD_data.xlsx')
glimpse(raw_adhd)
```

The data set is mostly compromised of "dbl" type, even though some of the columns such as Sex and Race are prime candidates to be converted to factors. There are unique values for 'Age' and 'Initial'. Since the column 'Initial' is a sort of identifier, we can safely remove it, the column is not necessary for analysis.

It's important to examine the data dictionary and see which columns might be combined or separated into different data sets.

The original dataframe contains 175 observations (i.e. survey participants) as rows and 54 columns as variables.
The columns contain both qualitative and quantitative variables.
Moreover, some columns represent categorical data but is encoded as numerical values.


------------------------------------------------------------------------------------------------------------------------------
Data Dictionary:
Sex: Male - 1, Female - 2
Race: Race:  White-1, African American-2, Hispanic-3, Asian-4, Native American-5, Other or missing data -6
ADHD self-report scale: Never-0, rarely-1, sometimes-2, often-3, very often-4
Mood disorder questions: No-0, yes-1; question 3: no problem-0, minor-1, moderate-2, serious-3
Individual substances misuse:  no use-0, use-1, abuse-2, dependence-3
Court Order:  No-0, Yes-1
Education: 1-12 grade, 13+ college
History of Violence: No-0, Yes-1
Disorderly Conduct: No-0, Yes-1
Suicide attempt: No-0, Yes-1
Abuse Hx: No-0, Physical (P)-1, Sexual (S)-2, Emotional (E)-3, P&S-4, P&E-5, S&E-6, P&S&E-7
Non-substance-related Dx: 0 – none; 1 – one; 2 – More than one
Substance-related Dx: 0 – none; 1 – one Substance-related; 2 – two; 3 – three or more
Psychiatric Meds: 0 – none; 1 – one psychotropic med; 2 – more than one psychotropic med
------------------------------------------------------------------------------------------------------------------------------

Identifying the missing data from the data set:

```{r}

plot_missing(
  raw_adhd,
  group = list(Good = 0.05, OK = 0.4, Bad = 0.8, Remove = 1),
  missing_only = TRUE,
  geom_label_args = list(),
  title = NULL,
  ggtheme = theme_gray(),
  theme_config = list(legend.position = c("bottom"))
)

```

There are columns missing data, with the exception of Psychiatric Meds, most of the missing data appears imputable. The 'Psychiatric Meds' column is missing 67.43% of data points. Imputing the data for this column is not optimal since most of the data would be imputed.  

```{r}
gg_miss_upset(raw_adhd)
```


```{r message=FALSE, warning=FALSE}
# rename data column names
raw_adhd <- raw_adhd %>% dplyr::rename_all(funs(make.names(.)))
```


```{r message=FALSE, warning=FALSE}
imputed_Data <- mice(raw_adhd, m=2, maxit = 2, method = 'cart', seed = 500)
```
```{r}
raw_adhd <- complete(imputed_Data,2)
```


```{r}
library(skimr)
skim(raw_adhd)
```

#### Data Cleaning 

```{r}
#Remove the Psychiatric Medicine and Initial Columns from data set.
adhd_df <- raw_adhd[-c(1, 54)]

```

#### Data Exploration

The various distribution of the responses to the ADHD questionnaire are displayed below. Also Distribution of Binary and Ordinal Variables.

```{r}
plot_histogram(adhd_df, ggtheme = theme_linedraw())

```
After observing the plots to assess how the distributions involved in the dataset. Based on the histrograms plotted above, we can note that there are many observations although numeric, behave as categorical features and this will need to be assessed when performing the kmeans clustering analysis. There does not seem to be any clear distinguishable outliers however there does seem to be some features that experience low variance such Stimulants where majority of the recorded observations are 0.


#### Data Transformation / Further Data Exploration

The columns Sex and Age are going to be converted to factors. When building future models, it's easier to read the different clusters or groups with certain categorical data turned into factors with appropriate labels. 

```{r}

adhd_df$Sex <-factor(adhd_df$Sex, levels = c(1,2), labels=c('Male','Female'))

adhd_df$Race <-factor(adhd_df$Race, levels=c(1,2,3,4,5,6), labels = c('White', 'African American', 'Hispanic', 'Asian', 'Native American', 'Other'))

```

```{r}
adhd_df[sapply(adhd_df, is.double)] <- lapply(adhd_df[sapply(adhd_df, is.double)], as.factor)

```

```{r}
glimpse(adhd_df)
```

### Clustering

Clustering is the most common form of unsupervised learning. It's a machine learning algorithm used to draw inferences from unlabeled data. The algorithm groups a set of data points into subsets. The goal is to create clusters that are have minimal variance internally, but maximum variance from external clusters. 

There are methods to clustering data. One is agglomerative, each observation in a respective cluster then merging together until a stop criteria is reached. The second is divisive, with all observations in a single cluster and subsequent splitting occures until a stop criteria is met. 

#### k-means clustering

K-means clustering is one of the simplest and popular unsupervised machine learning algorithms. Typically, unsupervised algorithms make inferences from datasets using only input vectors without referring to known, or labelled, outcomes.

To perform a cluster analysis in R, generally, the data should be prepared as follows:

- Removal totalized features
- Numeric to Factor Conversions
- Newly transformed categorical variables were binarized into 0/1. k-means will not be able to distinguish the eucliiand distances properly between classes that span more than 2 categories. 

- Normalization: features need to be normalized such that the distances they are centered and scaled the mean is 0 and the Stdev is 1, this scales all the data to allow kmeans to appropriately place centroids and observations at appropriate distances.

- Colinearity test: Colinearity was tested and it was determined that there was not sufficient colinearity between any variables such that they needed to be removed for this reason alone.

- Removed low-variance features: From Data Exploration section Stimulants seems like a low-variance variable with majority of categories recorded at 0. 


```{r}
adhd_df %>% recipe(~.) %>% 
  step_rm(contains("total")) %>% 
  #step_mutate_at(-Age, fn = ~ as.factor(.)) %>% 
  step_dummy(all_nominal(), one_hot = T) %>% 
  step_normalize(all_predictors()) %>%
  step_nzv(all_predictors()) %>% 
  step_corr(all_predictors()) %>% 
  prep() #%>% 
```
The classification of observations into groups requires some methods for computing the distance or the (dis)similarity between each pair of observations. 
The choice of distance measures is a critical step in clustering. It defines how the similarity of two elements (x, y) is calculated and it will influence the shape of the clusters. The choice also has has a strong influence on the clustering results.

the default distance measure is the Euclidean distance. However, depending on the type of the data and the research questions, other dissimilarity measures might be preferred and you should be aware of the options.

Within R it is simple to compute and visualize the distance matrix using the functions get_dist and fviz_dist from the factoextra R package.

```{r}
distance <- get_dist(adhd_df)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

```


```{r}
df <- na.omit(adhd_df)
k2 <- kmeans(df, centers = 2, nstart = 25)
k3 <- kmeans(df, centers = 3, nstart = 25)
k4 <- kmeans(df, centers = 4, nstart = 25)
k5 <- kmeans(df, centers = 5, nstart = 25)
str(k2)
```
We can also view our results by using fviz_cluster. This provides a nice illustration of the clusters. If there are more than two dimensions (variables) fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance.

Because the number of clusters (k) must be set before we start the algorithm, it is often advantageous to use several different values of k and examine the differences in the results. We can execute the same process for 3, 4, and 5 clusters, and the results are shown in the figure:

```{r}
# I had to convert df back to numeric to plot the clusters

df[sapply(df, is.factor)] <- lapply(df[sapply(df, is.factor)], as.numeric)

p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

**Determining Optimal Clusters**

As you may recall the analyst specifies the number of clusters to use; preferably the analyst would like to use the optimal number of clusters. To aid the analyst, the following explains the three most popular methods for determining the optimal clusters, which includes:

- Elbow method
- Silhouette method
- Gap statistic

Recall that, the basic idea behind cluster partitioning methods, such as k-means clustering, is to define clusters such that the total intra-cluster variation (known as total within-cluster variation or total within-cluster sum of square) is minimized

The total within-cluster sum of square (wss) measures the compactness of the clustering and we want it to be as small as possible. Thus, we can use the following algorithm to define the optimal clusters:

- Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters
- For each k, calculate the total within-cluster sum of square (wss)
- Plot the curve of wss according to the number of clusters k.
- The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.


Fortunately, this process to compute the “Elbow method” has been wrapped up in a single function (fviz_nbclust):

```{r}
set.seed(123)

fviz_nbclust(df, kmeans, method = "wss")
```
The results suggest that 4 is the optimal number of clusters as it appears to be the bend in the knee (or elbow).

**Extracting Results**

With most of these approaches suggesting 2 as the number of optimal clusters, we can perform the final analysis and extract the results using 2 clusters.

```{r}
set.seed(123)
final <- kmeans(df, 2, nstart = 25)
fviz_cluster(final, data = df)
```


#### Hierarchical Clustering

Hierarchical Clustering works by repeatingly combining the two nearest clusters into a larger cluster. It is a bottom-up approach which doesn't require the specification of the number of clusters beforehand. 

The final structure of the cluster is represented by a dendrogram diagram. 

In order to perform hierarchical clustering we will perform the following:

1. Normalize our continuous numerical values.Each observations feature values are presented as coordinates in n-dimensional space (n being the number of predictors/features). The distances between coordinates will be calculated in step 2 using a standard of measure. However, if the coordinates are normalized it may lead to false results.  
2. Since we have binary data, as well as categorical data, we can use the Jaccard distance measure. 



```{r}
set.seed(300)
#scale the numerical continuous data
adhd1 %>% select_if( is.numeric) %>% lapply(scale) ->adhd1

adhd1 <- as.data.frame(adhd1)
adhd2 <- adhd1
adhd2$Sex <- adhd$Sex
adhd2$Race <- adhd$Race
```

```{r}
#compute the distance
dist_mat <- dist(adhd2, method = 'euclidean')
```

```{r}
#select linkage method

hclust_comp <- hclust(dist_mat, method = 'complete')
plot(hclust_comp)
```

```{r}
#View the different clusters
plot(hclust_avg)
rect.hclust(hclust_avg , k = 4, border = 2:6)
abline(h = 13.8, col = 'red')
```

#### Question 2 Principal Component Analysis (PCA)

PCA commonly used for dimensionality reduction by using each data point onto only the first few principal components (most cases first and second dimensions) to obtain lower-dimensional data while keeping as much of the data’s variation as possible.

Features that are strongly correlated with each other are more suitable for PCA than those loosely related. Below corrplot using the Spearman correlation for the categorical variables demonstrate that the features within ADHD set are more strongly correlated than the MD set. In addition, there are too many missing values for the individual substance misuse, and therefore PCA is not performed on this set. For question 2, we conduct PCA on both ADHD and MD, but ADHD is demonstrated to be more suitable for the task.

##### PCA - ADHD

```{r}
# pca - ADHD
PCA_adhd <- prcomp(df %>% dplyr::select(contains("adhd.")))

```

```{r}
sd <- PCA_adhd$sdev
loadings <- PCA_adhd$rotation
rownames(loadings) <- colnames(df %>% dplyr::select(contains("adhd.")))
scores <- PCA_adhd$x

summary(PCA_adhd)
```

We will visualize the results using scree and cumulative variance plot. The scree plot (“elbow” method)indicate that the first 4 components are the most important ones, although they merely captured roughly 32% of the variance in the data. The first 29 components have the standard deviation above 1 and together they captured 77% of the variance. The number of dimensions is significantly reduced from 91 to 29 in this case.

```{r}
par(mfrow = c(1, 2))

# scree plot
screeplot(PCA_adhd, type = "l", npcs = 30, main = "Screeplot of the first 30 PCs")
abline(h = 1, col = "red", lty = 5)
legend("topright", legend = c("Eigenvalue = 1"), col = c("red"), lty = 5, cex = 0.6)

# cumulative variance plot
cumpro <- cumsum(PCA_adhd$sdev^2 / sum(PCA_adhd$sdev^2))
plot(cumpro[0:30], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
```
**Loading**

Loadings are interpreted as the coefficients of the linear combination of the initial variables from which the principal components are constructed. From a numerical point of view, the loadings are equal to the coordinates of the variables divided by the square root of the eigenvalue associated with the component.

adhd_total is the most important contributor to the first principal component. PC1 is made up by majority of "_X4" variables, meaning the response of “very often” from the ADHD questions; whereas PC2 is made up by majority of "_X0" variables, meaning the response of “never” from the ADHD questions. As expected, PCA successfully extract components from features that are not strongly associated to each other.

```{r}
library(kableExtra)
cut_off <- sqrt(1/ncol(df %>% dplyr::select(contains("adhd.")))) 

loadingsDf <- loadings %>% 
    as.data.frame() %>% 
    tibble::rownames_to_column() %>%
    dplyr::select(variables = rowname, everything())

pc1_important <- loadingsDf %>% 
    dplyr::filter(abs(PC1) > cut_off) %>%
    dplyr::select(variables, PC1) %>%
    arrange(desc(abs(PC1)))

pc1_important %>% 
  kable() %>%
  scroll_box()
```

The biplot displays the individuals and variables in the same plot featuring the first two principal components. It seems to suggest that the individuals can be visually clustered into 4 groups based on their responses to the ADHD questions.


```{r}
biplot(PCA_adhd, scale = 0, cex = 0.5)
```

```{r}

```


